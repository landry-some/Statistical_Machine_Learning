# Machine Learning Study Notebooks

This repository contains practical Jupyter notebooks covering core machine learning concepts, algorithms, optimization techniques, and model evaluation methods.

The goal of this project is to build and experiment with fundamental ML algorithms while understanding the mathematical intuition behind them.

---

## Topics Covered

### Supervised Learning
- Logistic Regression
- Multi-Class Logistic Regression
- Linear Regression (L1 & L2 Regularization)
- Decision Trees
- Random Forest
- Support Vector Machines
- Naive Bayes (including Spam Filtering)

### Unsupervised Learning
- K-Means Clustering

### Optimization & Training
- Gradient Descent for Neural Networks
- Early Stopping
- Grid Search Parameter Tuning

### Feature Engineering
- Categorical Variables & One-Hot Encoding
- Count Vectorization

### Advanced Concepts
- Kernel Trick in SVM
- Regularization (L1 / L2)

---

## Structure

The repository primarily consists of Jupyter notebooks (.ipynb), each focused on a specific algorithm or concept.

Some notebooks include:
- Mathematical derivations
- Visualizations
- Practical experiments
- Model evaluation comparisons

---

## Requirements

- Python 3.x
- Jupyter Notebook or JupyterLab
- Common ML libraries such as:
  - NumPy
  - Pandas
  - Matplotlib
  - Scikit-learn

Install dependencies (if needed):

```bash
pip install numpy pandas matplotlib scikit-learn
```

---

## Usage

Start Jupyter:

```bash
jupyter notebook
```

Open any notebook and run cells sequentially to explore the implementation and experiments.

---

## Purpose

This repository is intended for:

- Learning machine learning fundamentals
- Practicing algorithm implementation
- Experimenting with hyperparameter tuning
- Understanding model evaluation techniques

---
